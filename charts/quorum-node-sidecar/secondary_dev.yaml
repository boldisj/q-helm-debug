---
# Source: quorum-node-sidecar/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-quorum-node-sidecar
  namespace: default
  labels:
    helm.sh/chart: quorum-node-sidecar-0.1.0
    app.kubernetes.io/name: release-name-quorum-node-sidecar
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "23.4"
    app.kubernetes.io/managed-by: Helm
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::838158121660:role/eleaflet-poc-secrets
---
# Source: quorum-node-sidecar/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-quorum-node-sidecar
  namespace: default
  labels:
    name: release-name-quorum-node-sidecar
data:
  nodekey: | 
    MGFhODFjYWIxYTAwMTUxZGY3OGI1ZWVkYWU4OTQ2M2Q3NzE3ODgyZDQwOTRiNzdmN2U5MzdlYWIzMGQxZWQyMw==
type: Opaque
---
# Source: quorum-node-sidecar/templates/configmap-scripts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-quorum-node-sidecar-scripts
  namespace: default
  labels:
    name: release-name-quorum-node-sidecar-scripts
data:
  ibft_propose.sh: |-
    #/bin/ash

    if [ $# -lt 1 ]; then
      echo " An address to vote in, or out, must be provided: "
      echo " ./propose_ibft.sh HEX_ADDRESS (true|false)"
    fi

    ADDRESS=$1
    VOTE_BOOL=true

    if [ $# -eq 2 ]; then
     VOTE_BOOL=$2
    fi
    RES=$(geth --exec "istanbul.propose(\"$1\", $VOTE_BOOL)" --cache=16 attach --datadir /quorum/home/dd /quorum/home/dd/geth.ipc)
    echo $RES
  ibft_propose_all.sh: |-
    #!/bin/ash
    #set -xe

    echo "1. Retrieving current validator candidates"
    echo "=========================================="
    /geth-helpers/geth-exec.sh "istanbul.candidates"

    echo ""
    echo "2. Proposing to remove existing validators that are unknown nodes"
    echo "================================================================="
    echo "> Retrieving current validators via 'istanbul.getValidators()' ..."
    # This will return ["0x....", "0x....", "0x...."]
    # Sample: ["0x3257b9c9f66c6ca013def70cc075313d9adc9ed6", "0x601a483538f17126eb78a1bec95d2a297f2b89dd", "0x6f0c0b4f77d74d04673be303e376faa01f358db8", "0xaabf92868622b738b81fd4ee5a8706e1cb43af2d", "0xba0a4116eb16cfb5bbbe00204ba017dbf2f88ee4", "0xc1c6048ee3079fe87c36bce542672e85a19fe967", "0xce75491ea8e89826a87b9dd6efbaa20d0f5ec67a", "0xfc9dfe9d2c1fb15c7105e56d2b5c925034d1b1e4"]
    validators=$(/geth-helpers/geth-exec.sh "istanbul.getValidators()")
    echo "  Current validators:"
    echo "    $validators"
    for each_validator in $(echo ${validators} | grep "0x" | sed 's/,//g; s/"//g; s/\[//g; s/\]//g' ); do
        echo ""
        echo "> Checking if $each_validator is in a known node ..."
        grep_result=$(grep -i "$each_validator" /quorum/home/istanbul-validator-config.toml/istanbul-validator-config.toml)
        if [[ "$grep_result" != "" ]]; then
            echo "    WELL-KNOWN: Validator $each_validator is well-known. No need to propose removal."
        else 
            echo "    UNKNOWN: Validator $each_validator is unknown and not listed in validator whitelist. Proposing to remove validator."
            /quorum/home/node-management/ibft_propose.sh $each_validator false
        fi
    done

    echo ""
    echo "3. Retrieving current validator candidates"
    echo "=========================================="
    /geth-helpers/geth-exec.sh "istanbul.candidates"

    echo ""
    echo "4. Proposing all known nodes as validators"
    echo "=========================================="
    for Addr in $( awk '/validators/,0' /quorum/home/istanbul-validator-config.toml/istanbul-validator-config.toml | grep "0x" | sed 's/,//g; s/"//g' ); do
        echo ""
        echo "> Proposing $Addr as a validator ..."
        /quorum/home/node-management/ibft_propose.sh $Addr true
    done


    echo ""
    echo "5. Retrieving current validator candidates"
    echo "=========================================="
    /geth-helpers/geth-exec.sh "istanbul.candidates"

  geth-attach.sh: |-
    #!/bin/sh

    # helper for connecting to geth from
    # outside the container
    # kubectl exec -it $POD -c quorum -- /geth-helpers/geth-attach.sh
    echo "connecting to geth /quorum/home"
    geth attach --datadir /quorum/home/dd /quorum/home/dd/geth.ipc
  geth-exec.sh: |-
    #!/bin/sh

    # helper for connecting to geth from
    # outside the container
    # kubectl exec -it $POD -c quorum -- /geth-helpers/geth-exec.sh "admin.peers.length"

    GETH_CMD="eth.blockNumber"
    if [ "$#" -gt 0 ]; then
      GETH_CMD=$1
    fi
    # see: https://github.com/ethereum/go-ethereum/pull/17281
    # https://github.com/ethereum/go-ethereum/issues/16905
    # to avoid warning being returned
    # "WARN [02-20|00:21:04.382] Sanitizing cache to Go's GC limits  provided=1024 updated=663"
    geth --exec $GETH_CMD --cache=16 attach --datadir /quorum/home/dd /quorum/home/dd/geth.ipc
---
# Source: quorum-node-sidecar/templates/configmap-settings.yaml
# public node key (aka node address) and used to generate istanbul-validator-config.toml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-quorum-node-sidecar-settings
  namespace: default
  labels:
    name: release-name-quorum-node-sidecar-settings
data:
  nodekey.pub: | 
    0xc10f1355e62F304D62F2815D98fDe1F54790FE87
  enode: | 
    a63f31d829e3ca30da9846b8891510eebff80ea9c775189dc0d7839acbb124623c182652ccff96439622313c5e61e7244fe26e597cb5dde472417ad00e8ed182
  permissioned-nodes.json: |
    [
      "enode://a63f31d829e3ca30da9846b8891510eebff80ea9c775189dc0d7839acbb124623c182652ccff96439622313c5e61e7244fe26e597cb5dde472417ad00e8ed182@release-name-quorum-node-sidecar-p2p:30303?discport=0",
      "enode://838a0bf2cee4dedc7753b5288ccb28595dcfbabb17eac571484aeba0377a8b8c6c30ab78703ca4d19db5d2d320abbda7ff95e5180b4db4cef75f1300d043e1ca@quorum-nlb-3cb3bf52f1017f0b.elb.us-east-1.amazonaws.com:30303?discport=0"
    ]
  istanbul-validator-config.toml: |-
    vanity = "0x00"
    validators = [
      "0xc10f1355e62F304D62F2815D98fDe1F54790FE87",
      "0x374c10643EAd88A57c707f9DAF91C3A99da04743"
    ]
  genesis-geth.json: |- 
    {
      "alloc": {
        "0x47e309034caeef40154a1b10f84439a915335bd9": {
            "balance": "1000000000000000000000000000"
        }
      },
      "coinbase": "0x0000000000000000000000000000000000000000",
      "config": {
        "homesteadBlock": 0,
        "byzantiumBlock": 0,
        "constantinopleBlock": 0,
        "petersburgBlock": 0,
        "istanbulBlock": 0,
        "eip150Block": 0,
        "eip150Hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
        "eip155Block": 0,
        "eip158Block": 0,
        "maxCodeSizeConfig": [
          {
            "block": 0,
            "size": 32
          }
        ],
        "chainId": 10,
        "isQuorum": true,
        "istanbul": {
          "epoch": 30000,
          "policy": 0
        }
      },
      "difficulty": "0x1",
      "extraData": "0x0000000000000000000000000000000000000000000000000000000000000000d8d594374c10643ead88a57c707f9daf91c3a99da0474380c0",
      "gasLimit": "0xE0000000",
      "mixHash": "0x63746963616c2062797a616e74696e65206661756c7420746f6c6572616e6365",
      "nonce": "0x0",
      "parentHash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "timestamp": "0x00"
    }
---
# Source: quorum-node-sidecar/templates/pvc-data.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: release-name-quorum-node-sidecar-data
  namespace: default
  labels:
    helm.sh/chart: quorum-node-sidecar-0.1.0
    app.kubernetes.io/name: release-name-quorum-node-sidecar
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "23.4"
    app.kubernetes.io/managed-by: Helm
  finalizers: 
    - kubernetes.io/pvc-protection
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "3Gi"
---
# Source: quorum-node-sidecar/templates/pvc-logs.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: release-name-quorum-node-sidecar-logs
  namespace: default
  labels:
    helm.sh/chart: quorum-node-sidecar-0.1.0
    app.kubernetes.io/name: release-name-quorum-node-sidecar
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "23.4"
    app.kubernetes.io/managed-by: Helm
  finalizers: 
    - kubernetes.io/pvc-protection
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "1Gi"
---
# Source: quorum-node-sidecar/templates/pvc-sidecar.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: "pvc-quorum-sidecar"
  namespace: default
  labels:
    helm.sh/chart: quorum-node-sidecar-0.1.0
    app.kubernetes.io/name: release-name-quorum-node-sidecar
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "23.4"
    app.kubernetes.io/managed-by: Helm
  finalizers: 
    - kubernetes.io/pvc-protection
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "1Gi"
---
# Source: quorum-node-sidecar/templates/service-p2p.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-quorum-node-sidecar-p2p
  namespace: default
  labels: 
    helm.sh/chart: quorum-node-sidecar-0.1.0
    app.kubernetes.io/name: release-name-quorum-node-sidecar
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "23.4"
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-attributes: load_balancing.cross_zone.enabled=true,deletion_protection.enabled=false
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: "2"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: "2"
    service.beta.kubernetes.io/aws-load-balancer-ip-address-type: ipv4
    service.beta.kubernetes.io/aws-load-balancer-name: quorum-nlb2
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
    service.beta.kubernetes.io/aws-load-balancer-scheme: internal
    service.beta.kubernetes.io/aws-load-balancer-subnets: subnet-076f9af1e398f1860, subnet-0a23cae4f7e05c2b6
    service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true,deregistration_delay.timeout_seconds=120,deregistration_delay.connection_termination.enabled=true,stickiness.enabled=true,stickiness.type=source_ip
    service.beta.kubernetes.io/aws-load-balancer-type: external
spec:
  selector:
    app.kubernetes.io/name: release-name-quorum-node-sidecar
    app.kubernetes.io/instance: release-name
  type: LoadBalancer
  ports:
    - name: p2p-listener
      protocol: TCP
      port: 30303
      targetPort: 30303
---
# Source: quorum-node-sidecar/templates/service-rpc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-quorum-node-sidecar-rpc
  namespace: default
  labels: 
    helm.sh/chart: quorum-node-sidecar-0.1.0
    app.kubernetes.io/name: release-name-quorum-node-sidecar
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "23.4"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: release-name-quorum-node-sidecar
    app.kubernetes.io/instance: release-name
  type: ClusterIP
  ports:
    - name: rpc-listener
      protocol: TCP
      port: 8545
      targetPort: 8545
---
# Source: quorum-node-sidecar/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-quorum-node-sidecar
  namespace: default
  labels:
    helm.sh/chart: quorum-node-sidecar-0.1.0
    app.kubernetes.io/name: release-name-quorum-node-sidecar
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "23.4"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: release-name-quorum-node-sidecar
      app.kubernetes.io/instance: release-name
  strategy: 
    type: Recreate
  template:
    metadata:
      name: release-name-quorum-node-sidecar
      labels:
        app.kubernetes.io/name: release-name-quorum-node-sidecar
        app.kubernetes.io/instance: release-name
      # https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments
      annotations:
        checksum/secret: 8245a20c14be624d15fb83c046d80bd54d64a85a086c1cd7a135c0b17c07a7a7
        checksum/scripts: f5bfd4034623989f52514f912585f8f80b1d55eeef9185348885c76fe1d1e25b
        checksum/settings: fde694f67471b0a18f46320a2a6e99d00337e5f2841d9cb333c3a01c32dafb8e
        prometheus.io/scrape: "true"
        prometheus.io/path: "/debug/metrics/prometheus"
        prometheus.io/port: "9545"
    spec:
      automountServiceAccountToken: false
      serviceAccountName: release-name-quorum-node-sidecar
      securityContext:
        fsGroup: 10000
        runAsGroup: 10000
        runAsUser: 10000
      initContainers:
        - name: quorum-genesis-init
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 10000
            runAsNonRoot: true
            runAsUser: 10000
          image: "base-image-dev.dock.merck.com/eleaflet/quorum/23.4.0:build-179@sha256:4132d363d9734d5f097e980e64fc05f01f628885b2967c5ae1accd116f1cbef4"
          command:
            - sh
            - -cx
          args:
            - |
              if [ ! -f /quorum/home/dd/genesis_created ]; then
                /usr/local/bin/geth --datadir /quorum/home/dd init /quorum/home/genesis/genesis-geth.json
                date > /quorum/home/dd/genesis_created
              fi
          volumeMounts:
            - name: pvc-data
              mountPath: /quorum/home
              subPath: release-name-quorum-node-sidecar
            - name: cm-settings
              mountPath: /quorum/home/genesis/genesis-geth.json
              subPath: genesis-geth.json
              readOnly: true
      containers:
        - name: quorum
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 10000
            runAsNonRoot: true
            runAsUser: 10000
          image: "base-image-dev.dock.merck.com/eleaflet/quorum/23.4.0:build-179@sha256:4132d363d9734d5f097e980e64fc05f01f628885b2967c5ae1accd116f1cbef4"
          imagePullPolicy: Always
          readinessProbe:
            exec:
              command:
                - ls
                - /quorum/home/dd/geth.ipc
            initialDelaySeconds: 20
            periodSeconds: 3
          command:
            - sh
            - -cx
          args:
            - |
              # TODO: Not needed? JQ seems not to be used anywhere!
              # apk add jq

              ln -sf /quorum/home/permission-nodes/permissioned-nodes.json /quorum/home/dd/permissioned-nodes.json
              ln -sf /quorum/home/permission-nodes/permissioned-nodes.json /quorum/home/dd/static-nodes.json
              cat /quorum/home/genesis/genesis-geth.json
              chmod 644 /quorum/home/dd/keystore/key
              touch /quorum/home/dd/password.txt
              /usr/local/bin/geth \
                --datadir /quorum/home/dd \
                --nodiscover \
                --permissioned \
                --nat=none \
                --verbosity 5 \
                --emitcheckpoints \
                --gcmode archive \
                --istanbul.blockperiod 3 --mine --miner.threads 1 \
                --syncmode full \
                --networkid 10 \
                --snapshot=false --txlookuplimit=0 --cache.preimages --rpc.allow-unprotected-txs \
                --metrics --metrics.addr "0.0.0.0" --metrics.port "9545" \
                --port 30303 \
                --http --http.addr 0.0.0.0 --http.port 8545 --http.api "admin,eth,debug,miner,net,txpool,personal,web3,istanbul" \
                --http.corsdomain "*" --http.vhosts "*" \
                2>&1 | tee -a /quorum/logs/quorum.log
          ports:
            - containerPort: 8545
            - containerPort: 30303
            - containerPort: 9545
          env:
            - name: PRIVATE_CONFIG
              value: "ignore"
            - name: THIS_NODE_ID
              value: release-name-quorum-node-sidecar
            - name: THIS_ENODE
              value: a63f31d829e3ca30da9846b8891510eebff80ea9c775189dc0d7839acbb124623c182652ccff96439622313c5e61e7244fe26e597cb5dde472417ad00e8ed182
          volumeMounts:
            - name: pvc-data
              mountPath: /quorum/home
              subPath: release-name-quorum-node-sidecar
            - name: pvc-logs
              mountPath: /quorum/logs
              subPath: release-name-quorum-node-sidecar-logs
            - name: secret
              mountPath: /quorum/home/dd/geth/nodekey
              subPath: nodekey
              readOnly: true
            - name: cm-settings
              mountPath: /quorum/home/genesis/genesis-geth.json
              subPath: genesis-geth.json
              readOnly: true
            - name: cm-settings
              mountPath: /quorum/home/dd/geth/enode
              subPath: enode
              readOnly: true
            - name: cm-settings
              mountPath: /quorum/home/permission-nodes/permissioned-nodes.json
              subPath: permissioned-nodes.json
              readOnly: true
            - name: cm-settings
              mountPath: /quorum/home/istanbul-validator-config.toml/istanbul-validator-config.toml
              subPath: istanbul-validator-config.toml
              readOnly: true
            - name: cm-scripts
              mountPath: /geth-helpers/geth-attach.sh
              subPath: geth-attach.sh
              readOnly: true
            - name: cm-scripts
              mountPath: /geth-helpers/geth-exec.sh
              subPath: geth-exec.sh
              readOnly: true
            - name: cm-scripts
              mountPath: /quorum/home/node-management/ibft_propose.sh
              subPath: ibft_propose.sh
              readOnly: true
            - name: cm-scripts
              mountPath: /quorum/home/node-management/ibft_propose_all.sh
              subPath: ibft_propose_all.sh
              readOnly: true
        
        - name: net-sidecar    
          image: "base-image-dev.dock.merck.com/eleaflet/network-tools:build-145"
          imagePullPolicy: Always
          ports:
          - containerPort: 1180
            name: http-port
          - containerPort: 11443
            name: https-port
          env:
          - name: HTTP_PORT
            value: "1180"
          - name: HTTPS_PORT
            value: "11443"  
          resources:
            requests:
              cpu: "1m"
              memory: "20Mi"
            limits:
              cpu: "10m"
              memory: "20Mi"
          securityContext:
            runAsUser: 0
            capabilities:
              add: ["NET_ADMIN"]
          volumeMounts:
            - name: pvc-sidecar
              mountPath: /home/log
              subPath: sidecar-logs    

      volumes:
        - name: pvc-data
          persistentVolumeClaim:
            claimName: release-name-quorum-node-sidecar-data
        - name: pvc-logs
          persistentVolumeClaim:
            claimName: release-name-quorum-node-sidecar-logs
        - name: pvc-sidecar
          persistentVolumeClaim:
            claimName: "pvc-quorum-sidecar"
        - name: secret
          secret:
            secretName: release-name-quorum-node-sidecar
            defaultMode: 0440  # mount as r
            items:
              - key: nodekey
                path: nodekey
        - name: cm-settings
          configMap:
            name: release-name-quorum-node-sidecar-settings
            defaultMode: 0440  # mount as r
            items:
              - key: enode
                path: enode
              - key: permissioned-nodes.json
                path: permissioned-nodes.json
              - key: istanbul-validator-config.toml
                path: istanbul-validator-config.toml
              - key: genesis-geth.json
                path: genesis-geth.json
        - name: cm-scripts
          configMap:
            name: release-name-quorum-node-sidecar-scripts
            defaultMode: 0550  # mount as rx
            items:
              - key: geth-attach.sh
                path: geth-attach.sh
              - key: geth-exec.sh
                path: geth-exec.sh
              - key: ibft_propose.sh
                path: ibft_propose.sh
              - key: ibft_propose_all.sh
                path: ibft_propose_all.sh
---
# Source: quorum-node-sidecar/templates/post-upgrade.propose-validators.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-quorum-node-sidecar-quorum-propose
  namespace: default
  labels:
    name: release-name-quorum-node-sidecar-quorum-propose
  annotations:
    "description": "ServiceAccount for quorum propose all validators and deployments restarts"
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded,hook-failed
    "helm.sh/hook-weight": "10"
---
# Source: quorum-node-sidecar/templates/post-upgrade.propose-validators.yaml
# TODO : add remove of validators in plugin, chart as sh ibft_un_propose_all.sh and job
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-quorum-node-sidecar-quorum-propose
  namespace: default
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded,hook-failed
    "helm.sh/hook-weight": "20"
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["list"]
  - apiGroups: [""]
    resources: ["pods/exec"]
    verbs: ["create","watch"]
  - apiGroups: ["apps"]
    resources: ["deployments"]
    resourceNames: ["release-name-quorum-node-sidecar"]
    verbs: ["get"]
---
# Source: quorum-node-sidecar/templates/post-upgrade.propose-validators.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-quorum-node-sidecar-quorum-propose
  namespace: default
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded,hook-failed
    "helm.sh/hook-weight": "30"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-quorum-node-sidecar-quorum-propose
subjects:
  - kind: ServiceAccount
    name: release-name-quorum-node-sidecar-quorum-propose
    namespace: default
---
# Source: quorum-node-sidecar/templates/post-upgrade.propose-validators.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-quorum-node-sidecar-quorum-propose
  namespace: default
  labels:
    app: release-name-quorum-node-sidecar-quorum-propose
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "100"
spec:
  backoffLimit: 4
  activeDeadlineSeconds: 120
  ttlSecondsAfterFinished: 300
  template:
    spec:
      serviceAccountName: release-name-quorum-node-sidecar-quorum-propose
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsUser: 65534
      restartPolicy: Never
      containers:
        - name: kubectl
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 128Mi
          image: "bitnami/kubectl:1.21.14@sha256:bba32da4e7d08ce099e40c573a2a5e4bdd8b34377a1453a69bbb6977a04e8825"
          imagePullPolicy: Always
          command:
            - sh
            - -cx
          args:
            - |
              # No need to scale down and up in order to reflect latest changes as the deployment reacts to changes in configmaps/secrets
              # See https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments
              #
              echo "Sleeping for short time to wait until container may have restarted"
              sleep 60
              echo "Propose all as validator..."
              kubectl exec --container=quorum -t deploy/release-name-quorum-node-sidecar -- /quorum/home/node-management/ibft_propose_all.sh
